{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regional Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "from warnings import warn\n",
    "\n",
    "import cf_xarray as cfxr\n",
    "import cordex as cx\n",
    "import dask\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import regionmask\n",
    "import xarray as xr\n",
    "import xesmf as xe\n",
    "from dask.distributed import Client\n",
    "from evaltools import obs\n",
    "from evaltools.obs import eobs_mapping\n",
    "from evaltools.source import get_source_collection, open_and_sort\n",
    "from evaltools.utils import short_iid\n",
    "\n",
    "# from evaltools.eval import regional_means\n",
    "\n",
    "\n",
    "dask.config.set(scheduler=\"single-threaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traverseDir(root):\n",
    "    for dirpath, dirnames, filenames in os.walk(root):\n",
    "        for file in filenames:\n",
    "            if file.endswith(\".nc\"):\n",
    "                yield os.path.join(dirpath, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regional_mean(ds, regions=None, weights=None, aggr=None):\n",
    "    \"\"\"\n",
    "    Compute the regional mean of a dataset over specified regions.\n",
    "\n",
    "    Parameters:\n",
    "    ds (xarray.Dataset): The dataset to compute the regional mean for.\n",
    "    regions (regionmask.Regions): The regions to compute the mean over.\n",
    "\n",
    "    Returns:\n",
    "    xarray.Dataset: The regional mean values.\n",
    "    \"\"\"\n",
    "    mask = 1.0\n",
    "    if weights is None:\n",
    "        weights = xr.ones_like(ds.lon)\n",
    "    if regions:\n",
    "        mask = regions.mask_3D(ds.lon, ds.lat, drop=False)\n",
    "    if aggr == \"mean\":\n",
    "        result = ds.cf.weighted(mask * weights).mean(dim=(\"X\", \"Y\"), skipna=True)\n",
    "    elif aggr == \"P95\":\n",
    "        ds = np.abs(ds)\n",
    "        ds = ds.where(mask)\n",
    "        result = ds.cf.quantile(0.95, dim=[\"X\", \"Y\"], skipna=True)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def regional_means(dsets, regions=None, aggr=None):\n",
    "    \"\"\"\n",
    "    Compute the regional means for multiple datasets over specified regions.\n",
    "\n",
    "    Parameters:\n",
    "    dsets (dict): A dictionary of datasets to compute the regional means for.\n",
    "    regions (regionmask.Regions): The regions to compute the means over.\n",
    "\n",
    "    Returns:\n",
    "    xarray.Dataset: The concatenated regional mean values for all datasets.\n",
    "    \"\"\"\n",
    "    concat_dim = xr.DataArray(list(dsets.keys()), dims=\"iid\", name=\"iid\")\n",
    "    return xr.concat(\n",
    "        [regional_mean(ds, regions, None, aggr) for ds in dsets.values()],\n",
    "        dim=concat_dim,\n",
    "        coords=\"minimal\",\n",
    "        compat=\"override\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(dashboard_address=\"localhost:8787\", threads_per_worker=1)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertices = {\n",
    "    \"CMIP6\": (\"vertices_lon\", \"vertices_lat\"),\n",
    "    \"CMIP5\": (\"lon_vertices\", \"lat_vertices\"),\n",
    "}\n",
    "\n",
    "\n",
    "def add_bounds(ds):\n",
    "    if \"longitude\" not in ds.cf.bounds and \"latitude\" not in ds.cf.bounds:\n",
    "        ds = cx.transform_bounds(ds, trg_dims=(\"vertices_lon\", \"vertices_lat\"))\n",
    "    lon_bounds = ds.cf.get_bounds(\"longitude\")\n",
    "    lat_bounds = ds.cf.get_bounds(\"latitude\")\n",
    "    bounds_dim = [dim for dim in lon_bounds.dims if dim not in ds.indexes][0]\n",
    "    # reshape bounds for xesmf\n",
    "    ds = ds.assign_coords(\n",
    "        lon_b=cfxr.bounds_to_vertices(\n",
    "            lon_bounds, bounds_dim=bounds_dim, order=\"counterclockwise\"\n",
    "        ),\n",
    "        lat_b=cfxr.bounds_to_vertices(\n",
    "            lat_bounds, bounds_dim=bounds_dim, order=\"counterclockwise\"\n",
    "        ),\n",
    "    )\n",
    "    return ds\n",
    "\n",
    "\n",
    "def mask_with_sftlf(ds, sftlf=None):\n",
    "    if sftlf is None and \"sftlf\" in ds:\n",
    "        sftlf = ds[\"sftlf\"]\n",
    "        for var in ds.data_vars:\n",
    "            if var != \"sftlf\":\n",
    "                ds[var] = ds[var].where(sftlf > 0)\n",
    "        ds[\"mask\"] = sftlf > 0\n",
    "    else:\n",
    "        warn(f\"sftlf not found in dataset: {ds.source_id}\")\n",
    "    return ds\n",
    "\n",
    "\n",
    "def open_datasets(\n",
    "    variables, frequency=\"mon\", mask=True, add_missing_bounds=True, **kwargs\n",
    "):\n",
    "    cat = get_source_collection(\n",
    "        variables, frequency, add_fx=[\"areacella\", \"sftlf\"], **kwargs\n",
    "    )\n",
    "    dsets = open_and_sort(cat, merge_fx=True, apply_fixes=True)\n",
    "    if mask is True:\n",
    "        for ds in dsets.values():\n",
    "            mask_with_sftlf(ds)\n",
    "    if add_missing_bounds is True:\n",
    "        for dset_id, ds in dsets.items():\n",
    "            dsets[dset_id] = add_bounds(ds)\n",
    "    return dsets\n",
    "\n",
    "\n",
    "def create_cordex_grid(domain_id, **kwargs):\n",
    "    grid = cx.domain(domain_id, bounds=True, **kwargs)\n",
    "    # grid[\"lon\"].attrs = {}\n",
    "    # grid[\"vertices_lat\"].attrs = {}\n",
    "    lon_b = cfxr.bounds_to_vertices(\n",
    "        grid.cf.get_bounds(\"lon\"), bounds_dim=\"vertices\", order=\"counterclockwise\"\n",
    "    )\n",
    "    lat_b = cfxr.bounds_to_vertices(\n",
    "        grid.cf.get_bounds(\"lat\"), bounds_dim=\"vertices\", order=\"counterclockwise\"\n",
    "    )\n",
    "    return grid.assign_coords(lon_b=lon_b, lat_b=lat_b)\n",
    "\n",
    "\n",
    "def create_regridder(source, target, method=\"bilinear\"):\n",
    "    regridder = xe.Regridder(source, target, method=method)\n",
    "    return regridder\n",
    "\n",
    "\n",
    "def regrid(ds, regridder):\n",
    "    ds_regrid = regridder(ds)\n",
    "    for var in ds.data_vars:\n",
    "        if var not in [\"mask\", \"sftlf\"]:\n",
    "            ds_regrid[var] = ds_regrid[var].where(ds_regrid[\"mask\"] > 0.0)\n",
    "    return ds_regrid\n",
    "\n",
    "\n",
    "def regrid_dsets(dsets, target_grid, method=\"bilinear\"):\n",
    "    for dset_id, ds in dsets.items():\n",
    "        print(dset_id)\n",
    "        mapping = ds.cf[\"grid_mapping\"].grid_mapping_name\n",
    "        if mapping == \"rotated_latitude_longitude\":\n",
    "            dsets[dset_id] = ds.cx.rewrite_coords(coords=\"all\")\n",
    "        else:\n",
    "            print(f\"regridding {dset_id} with grid_mapping: {mapping}\")\n",
    "            regridder = create_regridder(ds, target_grid, method=method)\n",
    "            print(regridder)\n",
    "            dsets[dset_id] = regrid(ds, regridder)\n",
    "    return dsets\n",
    "\n",
    "\n",
    "def mask_invalid(ds, vars=None, threshold=0.1):\n",
    "    if isinstance(vars, str):\n",
    "        vars = [vars]\n",
    "    if vars is None:\n",
    "        var = list(ds.data_vars)\n",
    "    for var in vars:\n",
    "        var_nan = ds[var].isnull().sum(dim=\"time\") / ds.time.size\n",
    "        ds[var] = ds[var].where(var_nan < threshold)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_unit(ds, variable):\n",
    "    if variable == \"tas\":\n",
    "        ds = convert_celsius_to_kelvin(ds)\n",
    "    elif variable == \"pr\":\n",
    "        ds = convert_precipitation_to_mm(ds)\n",
    "    return ds\n",
    "\n",
    "\n",
    "def convert_celsius_to_kelvin(ds, threshold=200):\n",
    "    \"\"\"\n",
    "    Converts all temperature variables in an xarray Dataset from degrees Celsius to Kelvin\n",
    "    based on the 'units' attribute, value magnitude, or 'standard_name' attribute.\n",
    "\n",
    "    Parameters:\n",
    "        ds (xarray.Dataset): The input dataset.\n",
    "        threshold (float): A heuristic threshold (default=200) to assume temperatures\n",
    "                           below this value might be in Celsius.\n",
    "\n",
    "    Returns:\n",
    "        xarray.Dataset: A new dataset with converted temperature values.\n",
    "    \"\"\"\n",
    "    ds = ds.copy()  # Avoid modifying the original dataset\n",
    "\n",
    "    for var in ds.data_vars:\n",
    "        units = ds[var].attrs.get(\"units\", \"\").lower()\n",
    "        standard_name = ds[var].attrs.get(\"standard_name\", \"\").lower()\n",
    "\n",
    "        # Check if units explicitly indicate Celsius\n",
    "        if units in [\"c\", \"°c\", \"celsius\", \"degc\"]:\n",
    "            ds[var] = ds[var] + 273.15\n",
    "            ds[var].attrs[\"units\"] = \"K\"\n",
    "            print(\"Convert celsius to kelvin\")\n",
    "\n",
    "        # If no unit attribute exists, check standard_name for temperature-related terms\n",
    "        elif standard_name in [\n",
    "            \"air_temperature\",\n",
    "            \"sea_surface_temperature\",\n",
    "            \"surface_temperature\",\n",
    "        ]:\n",
    "            data_vals = ds[var].values\n",
    "            if np.nanmax(data_vals) < threshold:  # Likely in °C\n",
    "                ds[var] = ds[var] + 273.15\n",
    "                ds[var].attrs[\"units\"] = \"K\"\n",
    "                print(\"Convert celsius to kelvin\")\n",
    "\n",
    "    return ds\n",
    "\n",
    "\n",
    "def convert_precipitation_to_mm(ds):\n",
    "    \"\"\"\n",
    "    Converts all precipitation variables in an xarray Dataset to millimeters (mm)\n",
    "    based on the 'units' attribute or 'standard_name' attribute.\n",
    "\n",
    "    Parameters:\n",
    "        ds (xarray.Dataset): The input dataset.\n",
    "\n",
    "    Returns:\n",
    "        xarray.Dataset: A new dataset with converted precipitation values.\n",
    "    \"\"\"\n",
    "    ds = ds.copy()  # Avoid modifying the original dataset\n",
    "\n",
    "    for var in ds.data_vars:\n",
    "        units = ds[var].attrs.get(\"units\", \"\").lower()\n",
    "        # standard_name = ds[var].attrs.get(\"standard_name\", \"\").lower()\n",
    "\n",
    "        # Check if units explicitly indicate meters (m) or kilograms per meter per second squared (kg/m/s²)\n",
    "        if units in [\"m\", \"meters\"]:\n",
    "            ds[var] = ds[var] * 1000  # Convert from meters to millimeters\n",
    "            ds[var].attrs[\"units\"] = \"mm\"\n",
    "            print(\"Convert precipitation from meters to millimeters (mm).\")\n",
    "\n",
    "        elif units in [\"kg m-2 s-1\", \"kg/m/s2\"]:\n",
    "            # Precipitation rate in kg/m/s² can be converted to mm/s by multiplying by 1000\n",
    "            ds[var] = ds[var] * 86400  # Convert kg/m/s² to mm/s\n",
    "            ds[var].attrs[\"units\"] = \"mm\"\n",
    "            print(\"Convert precipitation from kg/m/s² to mm/s.\")\n",
    "\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seasonal_mean(da):\n",
    "    \"\"\"Optimized function to calculate seasonal averages from time series of monthly means\n",
    "\n",
    "    based on: https://xarray.pydata.org/en/stable/examples/monthly-means.html\n",
    "    \"\"\"\n",
    "    # Get number od days for each month\n",
    "    month_length = da.time.dt.days_in_month\n",
    "    # Calculate the weights by grouping by 'time.season'.\n",
    "    weights = (\n",
    "        month_length.groupby(\"time.season\") / month_length.groupby(\"time.season\").sum()\n",
    "    )\n",
    "\n",
    "    # Test that the sum of the weights for each season is 1.0\n",
    "    # np.testing.assert_allclose(weights.groupby(\"time.season\").sum().values, np.ones(4))\n",
    "\n",
    "    # Calculate the weighted average\n",
    "    return (\n",
    "        (da * weights).groupby(\"time.season\").sum(dim=\"time\", skipna=True, min_count=1)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_equal_period(ds, period):\n",
    "    years_in_ds = np.unique(ds.time.dt.year.values)\n",
    "    expected_years = np.arange(int(period.start), int(period.stop) + 1)\n",
    "    return np.array_equal(years_in_ds, expected_years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_360_longitudes(dataset: xr.Dataset, lonname: str = \"lon\") -> xr.Dataset:\n",
    "    \"\"\"\n",
    "    Fix longitude values.\n",
    "\n",
    "    Function to transform datasets where longitudes are in (0, 360) to (-180, 180).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset (xarray.Dataset): data stored by dimensions\n",
    "    lonname (str): name of the longitude dimension\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dataset (xarray.Dataset): data with the new longitudes\n",
    "    \"\"\"\n",
    "    lon = dataset[lonname]\n",
    "    if lon.max().values > 180 and lon.min().values >= 0:\n",
    "        dataset[lonname] = dataset[lonname].where(lon <= 180, other=lon - 360)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_attrs_ = [\n",
    "    \"project_id\",\n",
    "    \"domain_id\",\n",
    "    \"institution_id\",\n",
    "    \"driving_source_id\",\n",
    "    \"driving_experiment_id\",\n",
    "    \"driving_variant_label\",\n",
    "    \"source_id\",\n",
    "    \"version_realization\",\n",
    "    \"frequency\",\n",
    "    \"variable_id\",\n",
    "    \"version\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "var_dic = {\n",
    "    \"tas\": {\n",
    "        \"variable\": \"tas\",\n",
    "        \"name\": \"Temperature BIAS [K]\",\n",
    "        \"diff\": \"abs\",\n",
    "        \"range\": [-4, 4],\n",
    "        \"aggr\": \"mean\",\n",
    "        \"datasets\": [\"era5\", \"cerra\"],\n",
    "    },\n",
    "    \"pr\": {\n",
    "        \"variable\": \"pr\",\n",
    "        \"name\": \"Precipitation BIAS [%]\",\n",
    "        \"diff\": \"rel\",\n",
    "        \"range\": [-60, 180],\n",
    "        \"aggr\": \"mean\",\n",
    "        \"datasets\": [\"era5\", \"cerra-land\"],\n",
    "    },\n",
    "    \"tas95\": {\n",
    "        \"variable\": \"tas\",\n",
    "        \"name\": \"Temperature 95%-P [K]\",\n",
    "        \"diff\": \"abs\",\n",
    "        \"range\": [-2, 10],\n",
    "        \"aggr\": \"P95\",\n",
    "        \"datasets\": [\"era5\", \"cerra\"],\n",
    "    },\n",
    "    \"pr95\": {\n",
    "        \"variable\": \"pr\",\n",
    "        \"name\": \"Precipitation 95%-P [%]\",\n",
    "        \"diff\": \"rel\",\n",
    "        \"range\": [0, 400],\n",
    "        \"aggr\": \"P95\",\n",
    "        \"datasets\": [\"era5\", \"cerra-land\"],\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameter papermill\n",
    "index = \"pr\"\n",
    "frequency = \"mon\"\n",
    "domain = \"EUR-11\"\n",
    "regridding = \"bilinear\"\n",
    "period = slice(\"1989\", \"2008\")\n",
    "reference_regions = \"PRUDENCE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results_path = os.path.abspath(\n",
    "    os.path.join(os.getcwd(), \"..\", \"intermediate-results\")\n",
    ")\n",
    "save_figure_path = os.path.abspath(os.path.join(os.getcwd(), \"..\", \"plots\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "variable = var_dic[index][\"variable\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eur_colors = pd.read_csv(\"eurocordex_models.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prudence\n",
    "regions = regionmask.defined_regions.prudence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotated_grid = create_cordex_grid(\"EUR-11\", mip_era=\"CMIP5\")  # No matter CMIP5 or CMIP6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E-OBS is used as the reference dataset for all the analysis\n",
    "It is used to calculate bias not only respect to CORDEX, but also in comparison wit other reanalyses and observational dataset, to assess the uncertaintly of the observational dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load, regrid and calculate seasonal means\n",
    "eobs = obs.eobs(variables=\"rr\", add_mask=False).sel(time=period)\n",
    "eobs_var = [key for key, value in eobs_mapping.items() if value == variable][0]\n",
    "eobs = mask_invalid(eobs, vars=eobs_var, threshold=0.1)\n",
    "eobs = standardize_unit(eobs, variable)\n",
    "# eobs = load_eobs(add_mask=False, to_cf=False, variable = variable)\n",
    "# unmapped_to_nan, see https://github.com/pangeo-data/xESMF/issues/56\n",
    "regridder = xe.Regridder(eobs, rotated_grid, method=regridding, unmapped_to_nan=True)\n",
    "ref_on_rotated = regridder(eobs)\n",
    "if not check_equal_period(ref_on_rotated, period):\n",
    "    print(f\"Temporal coverage of dataset does not match with {period}\")\n",
    "ref_seasmean = seasonal_mean(ref_on_rotated[eobs_var].sel(time=period)).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_seasmean.plot(col=\"season\", col_wrap=2, cmap=\"jet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CERRA and ERA5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_mapping = {\n",
    "    \"cerra\": {\"tas\": \"t2m\", \"pr\": \"tp\"},\n",
    "    \"cerra-land\": {\"tas\": \"tas\", \"pr\": \"tp\"},\n",
    "    \"era5\": {\"tas\": \"t2m\", \"pr\": \"tp\"},\n",
    "}\n",
    "\n",
    "\n",
    "def load_obs(variable, dataset, add_mask=False):\n",
    "    root = f\"/mnt/CORDEX_CMIP6_tmp/aux_data/{dataset}/mon/{variable}/\"\n",
    "    ds = xr.open_mfdataset(\n",
    "        np.sort(list(traverseDir(root))), concat_dim=\"valid_time\", combine=\"nested\"\n",
    "    )\n",
    "    ds = ds.rename({\"valid_time\": \"time\"})\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsets = {}\n",
    "for dset in var_dic[variable][\"datasets\"]:\n",
    "    ds = load_obs(variable, dset)\n",
    "    ds = ds.sel(time=period).compute()\n",
    "    ds = fix_360_longitudes(ds, lonname=\"longitude\")\n",
    "    if not variable_mapping[dset][variable] == variable:\n",
    "        ds = ds.rename_vars({variable_mapping[dset][variable]: variable})\n",
    "    ds = standardize_unit(ds, variable)\n",
    "    dsets[dset] = ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dset in dsets.keys():\n",
    "    if not check_equal_period(dsets[dset], period):\n",
    "        print(f\"Temporal coverage of {dset} does not match with {period}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dset, ds in dsets.items():\n",
    "    regridder = xe.Regridder(ds, rotated_grid, method=regridding, unmapped_to_nan=False)\n",
    "    dsets[dset] = regridder(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if var_dic[variable][\"diff\"] == \"abs\":\n",
    "    diffs = {\n",
    "        dset_id: seasonal_mean(ds[[variable]].sel(time=period)).compute()\n",
    "        - (ref_seasmean)\n",
    "        for dset_id, ds in dsets.items()\n",
    "        if variable in ds.variables\n",
    "    }\n",
    "elif var_dic[variable][\"diff\"] == \"rel\":\n",
    "    diffs = {\n",
    "        dset_id: 100\n",
    "        * (seasonal_mean(ds[[variable]].sel(time=period)).compute() - (ref_seasmean))\n",
    "        / (ref_seasmean)\n",
    "        for dset_id, ds in dsets.items()\n",
    "        if variable in ds.variables\n",
    "    }\n",
    "obs_seasonal_bias = xr.concat(\n",
    "    list(diffs.values()),\n",
    "    dim=xr.DataArray(\n",
    "        list(map(lambda x: x, diffs.keys())),\n",
    "        dims=\"dset_id\",\n",
    "    ),\n",
    "    compat=\"override\",\n",
    "    coords=\"minimal\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_seasonal_bias[variable] = obs_seasonal_bias[variable].where(\n",
    "    (obs_seasonal_bias[variable] <= 1000) & (obs_seasonal_bias[variable] >= -1000)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_regions = regional_means(obs_seasonal_bias, regions, aggr=var_dic[index][\"aggr\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CMIP6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mip_era = \"CMIP6\"\n",
    "driving_source_id = \"ERA5\"\n",
    "# Define how to merge the files in xarray\n",
    "merge = [\"variable_id\", \"frequency\"]\n",
    "default_attrs = [d for d in default_attrs_ if d not in merge]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dsets = open_datasets(\n",
    "    [variable],\n",
    "    frequency=frequency,\n",
    "    driving_source_id=driving_source_id,\n",
    "    mask=True,\n",
    "    add_missing_bounds=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dset in dsets.keys():\n",
    "    if not check_equal_period(dsets[dset], period):\n",
    "        print(f\"Temporal coverage of {dset} does not match with {period}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dset in dsets.keys():\n",
    "    dsets[dset] = convert_celsius_to_kelvin(dsets[dset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsets = regrid_dsets(dsets, rotated_grid, method=regridding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if var_dic[index][\"diff\"] == \"abs\":\n",
    "    diffs = {\n",
    "        dset_id: seasonal_mean(ds[[variable]].sel(time=period)).compute()\n",
    "        - (ref_seasmean + 273.15)\n",
    "        for dset_id, ds in dsets.items()\n",
    "        if variable in ds.variables\n",
    "    }\n",
    "elif var_dic[index][\"diff\"] == \"rel\":\n",
    "    diffs = {\n",
    "        dset_id: 100\n",
    "        * (\n",
    "            seasonal_mean(ds[[variable]].sel(time=period)).compute() * 86400\n",
    "            - (ref_seasmean)\n",
    "        )\n",
    "        / (ref_seasmean)\n",
    "        for dset_id, ds in dsets.items()\n",
    "        if variable in ds.variables\n",
    "    }\n",
    "\n",
    "seasonal_bias = xr.concat(\n",
    "    list(diffs.values()),\n",
    "    dim=xr.DataArray(\n",
    "        list(\n",
    "            map(\n",
    "                lambda x: short_iid(x, [\"institution_id\", \"source_id\"], delimiter=\"-\"),\n",
    "                diffs.keys(),\n",
    "            )\n",
    "        ),\n",
    "        dims=\"dset_id\",\n",
    "    ),\n",
    "    compat=\"override\",\n",
    "    coords=\"minimal\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seasonal_bias[\"pr\"] = seasonal_bias[\"pr\"].where(\n",
    "    (seasonal_bias[\"pr\"] <= 1000) & (seasonal_bias[\"pr\"] >= -1000)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seasonal_bias[\"pr\"].isel(season=1, dset_id=3).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dset_id_regions = regional_means(seasonal_bias, regions, aggr=var_dic[index][\"aggr\"])\n",
    "dset_id_regions.to_netcdf(\n",
    "    f\"{save_results_path}/{index}_{mip_era}_{reference_regions}_{period.start}-{period.stop}.nc\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CMIP5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mip_era = \"CMIP5\"\n",
    "driving_source_id = \"ERAINT\"\n",
    "# Define how to merge the files in xarray\n",
    "merge = [\"variable_id\", \"frequency\", \"driving_variant_label\", \"version\"]\n",
    "default_attrs = [d for d in default_attrs_ if d not in merge]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsets = open_datasets(\n",
    "    [variable],\n",
    "    frequency=frequency,\n",
    "    driving_source_id=driving_source_id,\n",
    "    mask=True,\n",
    "    add_missing_bounds=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dset in dsets.keys():\n",
    "    if not check_equal_period(dsets[dset], period):\n",
    "        print(f\"Temporal coverage of {dset} does not match with {period}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dset in dsets.keys():\n",
    "    dsets[dset] = convert_celsius_to_kelvin(dsets[dset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsets = regrid_dsets(dsets, rotated_grid, method=regridding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if var_dic[index][\"diff\"] == \"abs\":\n",
    "    diffs = {\n",
    "        dset_id: seasonal_mean(ds[[variable]].sel(time=period)).compute()\n",
    "        - (ref_seasmean + 273.15)\n",
    "        for dset_id, ds in dsets.items()\n",
    "        if variable in ds.variables\n",
    "    }\n",
    "elif var_dic[index][\"diff\"] == \"rel\":\n",
    "    diffs = {\n",
    "        dset_id: 100\n",
    "        * (\n",
    "            seasonal_mean(ds[[variable]].sel(time=period)).compute() * 86400\n",
    "            - (ref_seasmean)\n",
    "        )\n",
    "        / (ref_seasmean)\n",
    "        for dset_id, ds in dsets.items()\n",
    "        if variable in ds.variables\n",
    "    }\n",
    "\n",
    "seasonal_bias = xr.concat(\n",
    "    list(diffs.values()),\n",
    "    dim=xr.DataArray(\n",
    "        list(\n",
    "            map(\n",
    "                lambda x: short_iid(x, [\"institution_id\", \"source_id\"], delimiter=\"-\"),\n",
    "                diffs.keys(),\n",
    "            )\n",
    "        ),\n",
    "        dims=\"dset_id\",\n",
    "    ),\n",
    "    compat=\"override\",\n",
    "    coords=\"minimal\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seasonal_bias[variable] = seasonal_bias[variable].where(\n",
    "    (seasonal_bias[variable] <= 1000) & (seasonal_bias[variable] >= -1000)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_id_regions = regional_means(seasonal_bias, regions, aggr=var_dic[index][\"aggr\"])\n",
    "dset_id_regions.to_netcdf(\n",
    "    f\"{save_results_path}/{index}_{mip_era}_{reference_regions}_{period.start}-{period.stop}.nc\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load results for both CMIP5 and CMIP6 simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seasons = [\"DJF\", \"MAM\", \"JJA\", \"SON\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_id_regions_CMIP6 = xr.open_dataset(\n",
    "    f\"{save_results_path}/{index}_CMIP6_{reference_regions}_{period.start}-{period.stop}.nc\"\n",
    ")\n",
    "dset_id_regions_CMIP5 = xr.open_dataset(\n",
    "    f\"{save_results_path}/{index}_CMIP5_{reference_regions}_{period.start}-{period.stop}.nc\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_CMIP6 = dset_id_regions_CMIP6.to_dataframe().reset_index()\n",
    "df_CMIP5 = dset_id_regions_CMIP5.to_dataframe().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_obs = obs_regions.to_dataframe().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.min(df_CMIP5[variable]), np.max(df_CMIP5[variable]))\n",
    "print(np.min(df_CMIP6[variable]), np.max(df_CMIP6[variable]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "regs = [\"EA\", \"IP\", \"ME\", \"SC\"]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8), sharex=True, sharey=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "handles = []\n",
    "labels = []\n",
    "\n",
    "for i, region in enumerate(regs):\n",
    "    ax = axes[i]\n",
    "\n",
    "    df_CMIP6_region = df_CMIP6[df_CMIP6[\"abbrevs\"] == region]\n",
    "    df_CMIP5_region = df_CMIP5[df_CMIP5[\"abbrevs\"] == region]\n",
    "    df_obs_region = df_obs[df_obs[\"abbrevs\"] == region]\n",
    "\n",
    "    df_CMIP6_region[\"season_num\"] = df_CMIP6_region[\"season\"].apply(\n",
    "        lambda x: seasons.index(x)\n",
    "    )\n",
    "    df_CMIP5_region[\"season_num\"] = df_CMIP5_region[\"season\"].apply(\n",
    "        lambda x: seasons.index(x)\n",
    "    )\n",
    "    df_obs_region[\"season_num\"] = df_obs_region[\"season\"].apply(\n",
    "        lambda x: seasons.index(x)\n",
    "    )\n",
    "\n",
    "    df_CMIP6_region[\"season_shifted\"] = df_CMIP6_region[\"season_num\"] + 0.1\n",
    "    df_CMIP5_region[\"season_shifted\"] = df_CMIP5_region[\"season_num\"] - 0.1\n",
    "    df_obs_region[\"season_shifted\"] = df_obs_region[\"season_num\"] + 0.3\n",
    "\n",
    "    # Create lists to store the bias values for calculating median\n",
    "    cmip6_biases = {season: [] for season in seasons}\n",
    "    cmip5_biases = {season: [] for season in seasons}\n",
    "\n",
    "    for idx, row in df_CMIP6_region.iterrows():\n",
    "        dset_id = row[\"dset_id\"]\n",
    "        color = eur_colors[\"color\"][eur_colors[\"model\"] == dset_id].values[0]\n",
    "        scatter = ax.scatter(\n",
    "            row[\"season_shifted\"],\n",
    "            row[variable],\n",
    "            color=color,\n",
    "            edgecolors=color,\n",
    "            marker=\"o\",\n",
    "            s=80,\n",
    "        )\n",
    "\n",
    "        # Collect bias values for median calculation\n",
    "        cmip6_biases[row[\"season\"]].append(abs(row[variable]))\n",
    "\n",
    "        if dset_id not in labels:\n",
    "            handles.append(scatter)\n",
    "            labels.append(dset_id)\n",
    "\n",
    "        parent = eur_colors[\"parent\"][eur_colors[\"model\"] == dset_id].values[0]\n",
    "        if not pd.isnull(parent):\n",
    "            row_cmip5 = df_CMIP5_region[df_CMIP5_region[\"dset_id\"] == parent]\n",
    "            if not row_cmip5.empty:\n",
    "                row_cmip5 = row_cmip5[row_cmip5[\"season\"] == row.season].iloc[0]\n",
    "                ax.plot(\n",
    "                    [row_cmip5[\"season_shifted\"], row[\"season_shifted\"]],\n",
    "                    [row_cmip5[variable], row[variable]],\n",
    "                    color=color,\n",
    "                    linestyle=\"-\",\n",
    "                    zorder=0,\n",
    "                )\n",
    "\n",
    "    for idx, row in df_CMIP5_region.iterrows():\n",
    "        dset_id = row[\"dset_id\"]\n",
    "        color = eur_colors[\"color\"][eur_colors[\"model\"] == dset_id].values[0]\n",
    "        scatter = ax.scatter(\n",
    "            row[\"season_shifted\"],\n",
    "            row[variable],\n",
    "            color=color,\n",
    "            edgecolors=color,\n",
    "            facecolor=\"none\",\n",
    "            marker=\"o\",\n",
    "            s=80,\n",
    "        )\n",
    "\n",
    "        # Collect bias values for median calculation\n",
    "        cmip5_biases[row[\"season\"]].append(abs(row[variable]))\n",
    "\n",
    "        if dset_id not in labels:\n",
    "            handles.append(scatter)\n",
    "            labels.append(dset_id)\n",
    "\n",
    "    for idx, row in df_obs_region.iterrows():\n",
    "        dset_id = row[\"dset_id\"]\n",
    "        if \"era5\" in dset_id:\n",
    "            color = \"magenta\"\n",
    "        else:\n",
    "            color = \"black\"\n",
    "        scatter = ax.scatter(\n",
    "            row[\"season_shifted\"],\n",
    "            row[variable],\n",
    "            color=color,\n",
    "            edgecolors=color,\n",
    "            facecolor=\"none\",\n",
    "            marker=\"^\",\n",
    "            s=80,\n",
    "        )\n",
    "\n",
    "        if dset_id not in labels:\n",
    "            handles.append(scatter)\n",
    "            labels.append(dset_id)\n",
    "\n",
    "    axes[0].set_ylabel(var_dic[index][\"name\"])\n",
    "    axes[2].set_ylabel(var_dic[index][\"name\"])\n",
    "    # Add region label in the top-left corner of each subplot\n",
    "    ax.text(\n",
    "        0.05,\n",
    "        0.95,\n",
    "        region,\n",
    "        transform=ax.transAxes,\n",
    "        fontsize=12,\n",
    "        verticalalignment=\"top\",\n",
    "        horizontalalignment=\"left\",\n",
    "        color=\"black\",\n",
    "        weight=\"bold\",\n",
    "    )\n",
    "\n",
    "    ax.set_xticks([0, 1, 2, 3])  # Adjust tick positions according to the shift\n",
    "    ax.set_xticklabels(seasons)  # Set the names of the seasons as labels\n",
    "\n",
    "    ax.grid(True)\n",
    "    ax.axhline(0, color=\"black\", linestyle=\"--\")\n",
    "\n",
    "    if index == \"pr\":\n",
    "        ax.fill_between([-0.5, 3.5], 0, 25, color=\"#cceeff\", alpha=0.5)\n",
    "\n",
    "    # Calculate and display the absolute median bias for each season for both CMIP5 and CMIP6\n",
    "    for j, season in enumerate(seasons):\n",
    "        cmip6_median = (\n",
    "            np.nanmedian(cmip6_biases[season]) if cmip6_biases[season] else np.nan\n",
    "        )\n",
    "        cmip5_median = (\n",
    "            np.nanmedian(cmip5_biases[season]) if cmip5_biases[season] else np.nan\n",
    "        )\n",
    "\n",
    "        # Add the absolute median bias text below the season labels\n",
    "        ax.text(\n",
    "            j,\n",
    "            0.01,  # x in data coords, y as fraction of axes height (just above bottom)\n",
    "            f\"{cmip5_median:.1f}  {cmip6_median:.1f}\",\n",
    "            fontsize=10,\n",
    "            verticalalignment=\"bottom\",\n",
    "            horizontalalignment=\"center\",\n",
    "            color=\"black\",\n",
    "            transform=ax.get_xaxis_transform(),  # y in axes coords, x in data coords\n",
    "        )\n",
    "#        ax.text(j, var_dic[index]['range'][0]+0.5, f'{cmip5_median:.1f}  {cmip6_median:.1f}',\n",
    "#                fontsize=10, verticalalignment='top', horizontalalignment='center', color='black')\n",
    "\n",
    "fig.legend(\n",
    "    handles,\n",
    "    labels,\n",
    "    loc=\"upper center\",\n",
    "    bbox_to_anchor=(0.5, -0.05),\n",
    "    ncol=5,\n",
    "    fontsize=10,\n",
    ")\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "fig.savefig(\n",
    "    f\"{save_figure_path}/{reference_regions}_A_bias_{index}_{period.start}-{period.stop}.png\",\n",
    "    bbox_inches=\"tight\",\n",
    "    transparent=True,\n",
    "    pad_inches=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "regs = [\"AL\", \"BI\", \"FR\", \"MD\"]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8), sharex=True, sharey=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "handles = []\n",
    "labels = []\n",
    "\n",
    "for i, region in enumerate(regs):\n",
    "    ax = axes[i]\n",
    "\n",
    "    df_CMIP6_region = df_CMIP6[df_CMIP6[\"abbrevs\"] == region]\n",
    "    df_CMIP5_region = df_CMIP5[df_CMIP5[\"abbrevs\"] == region]\n",
    "    df_obs_region = df_obs[df_obs[\"abbrevs\"] == region]\n",
    "\n",
    "    df_CMIP6_region[\"season_num\"] = df_CMIP6_region[\"season\"].apply(\n",
    "        lambda x: seasons.index(x)\n",
    "    )\n",
    "    df_CMIP5_region[\"season_num\"] = df_CMIP5_region[\"season\"].apply(\n",
    "        lambda x: seasons.index(x)\n",
    "    )\n",
    "    df_obs_region[\"season_num\"] = df_obs_region[\"season\"].apply(\n",
    "        lambda x: seasons.index(x)\n",
    "    )\n",
    "\n",
    "    df_CMIP6_region[\"season_shifted\"] = df_CMIP6_region[\"season_num\"] + 0.1\n",
    "    df_CMIP5_region[\"season_shifted\"] = df_CMIP5_region[\"season_num\"] - 0.1\n",
    "    df_obs_region[\"season_shifted\"] = df_obs_region[\"season_num\"] + 0.3\n",
    "\n",
    "    # Create lists to store the bias values for calculating median\n",
    "    cmip6_biases = {season: [] for season in seasons}\n",
    "    cmip5_biases = {season: [] for season in seasons}\n",
    "\n",
    "    for idx, row in df_CMIP6_region.iterrows():\n",
    "        dset_id = row[\"dset_id\"]\n",
    "        color = eur_colors[\"color\"][eur_colors[\"model\"] == dset_id].values[0]\n",
    "        scatter = ax.scatter(\n",
    "            row[\"season_shifted\"],\n",
    "            row[variable],\n",
    "            color=color,\n",
    "            edgecolors=color,\n",
    "            marker=\"o\",\n",
    "            s=80,\n",
    "        )\n",
    "\n",
    "        # Collect bias values for median calculation\n",
    "        cmip6_biases[row[\"season\"]].append(abs(row[variable]))\n",
    "\n",
    "        if dset_id not in labels:\n",
    "            handles.append(scatter)\n",
    "            labels.append(dset_id)\n",
    "\n",
    "        parent = eur_colors[\"parent\"][eur_colors[\"model\"] == dset_id].values[0]\n",
    "        if not pd.isnull(parent):\n",
    "            row_cmip5 = df_CMIP5_region[df_CMIP5_region[\"dset_id\"] == parent]\n",
    "            if not row_cmip5.empty:\n",
    "                row_cmip5 = row_cmip5[row_cmip5[\"season\"] == row.season].iloc[0]\n",
    "                ax.plot(\n",
    "                    [row_cmip5[\"season_shifted\"], row[\"season_shifted\"]],\n",
    "                    [row_cmip5[variable], row[variable]],\n",
    "                    color=color,\n",
    "                    linestyle=\"-\",\n",
    "                    zorder=0,\n",
    "                )\n",
    "\n",
    "    for idx, row in df_CMIP5_region.iterrows():\n",
    "        dset_id = row[\"dset_id\"]\n",
    "        color = eur_colors[\"color\"][eur_colors[\"model\"] == dset_id].values[0]\n",
    "        scatter = ax.scatter(\n",
    "            row[\"season_shifted\"],\n",
    "            row[variable],\n",
    "            color=color,\n",
    "            edgecolors=color,\n",
    "            facecolor=\"none\",\n",
    "            marker=\"o\",\n",
    "            s=80,\n",
    "        )\n",
    "\n",
    "        # Collect bias values for median calculation\n",
    "        cmip5_biases[row[\"season\"]].append(abs(row[variable]))\n",
    "\n",
    "        if dset_id not in labels:\n",
    "            handles.append(scatter)\n",
    "            labels.append(dset_id)\n",
    "\n",
    "    for idx, row in df_obs_region.iterrows():\n",
    "        dset_id = row[\"dset_id\"]\n",
    "        if \"era5\" in dset_id:\n",
    "            color = \"magenta\"\n",
    "        else:\n",
    "            color = \"black\"\n",
    "        scatter = ax.scatter(\n",
    "            row[\"season_shifted\"],\n",
    "            row[variable],\n",
    "            color=color,\n",
    "            edgecolors=color,\n",
    "            facecolor=\"none\",\n",
    "            marker=\"^\",\n",
    "            s=80,\n",
    "        )\n",
    "\n",
    "        if dset_id not in labels:\n",
    "            handles.append(scatter)\n",
    "            labels.append(dset_id)\n",
    "\n",
    "    axes[0].set_ylabel(var_dic[index][\"name\"])\n",
    "    axes[2].set_ylabel(var_dic[index][\"name\"])\n",
    "    # Add region label in the top-left corner of each subplot\n",
    "    ax.text(\n",
    "        0.05,\n",
    "        0.95,\n",
    "        region,\n",
    "        transform=ax.transAxes,\n",
    "        fontsize=12,\n",
    "        verticalalignment=\"top\",\n",
    "        horizontalalignment=\"left\",\n",
    "        color=\"black\",\n",
    "        weight=\"bold\",\n",
    "    )\n",
    "\n",
    "    ax.set_xticks([0, 1, 2, 3])  # Adjust tick positions according to the shift\n",
    "    ax.set_xticklabels(seasons)  # Set the names of the seasons as labels\n",
    "\n",
    "    ax.grid(True)\n",
    "    ax.axhline(0, color=\"black\", linestyle=\"--\")\n",
    "\n",
    "    if index == \"pr\":\n",
    "        ax.fill_between([-0.5, 3.5], 0, 25, color=\"#cceeff\", alpha=0.5)\n",
    "\n",
    "    # Calculate and display the absolute median bias for each season for both CMIP5 and CMIP6\n",
    "    for j, season in enumerate(seasons):\n",
    "        cmip6_median = (\n",
    "            np.nanmedian(cmip6_biases[season]) if cmip6_biases[season] else np.nan\n",
    "        )\n",
    "        cmip5_median = (\n",
    "            np.nanmedian(cmip5_biases[season]) if cmip5_biases[season] else np.nan\n",
    "        )\n",
    "\n",
    "        # Add the absolute median bias text below the season labels\n",
    "        ax.text(\n",
    "            j,\n",
    "            0.01,  # x in data coords, y as fraction of axes height (just above bottom)\n",
    "            f\"{cmip5_median:.1f}  {cmip6_median:.1f}\",\n",
    "            fontsize=10,\n",
    "            verticalalignment=\"bottom\",\n",
    "            horizontalalignment=\"center\",\n",
    "            color=\"black\",\n",
    "            transform=ax.get_xaxis_transform(),  # y in axes coords, x in data coords\n",
    "        )\n",
    "#        ax.text(j, var_dic[index]['range'][0]+0.5, f'{cmip5_median:.1f}  {cmip6_median:.1f}',\n",
    "#                fontsize=10, verticalalignment='top', horizontalalignment='center', color='black')\n",
    "\n",
    "fig.legend(\n",
    "    handles,\n",
    "    labels,\n",
    "    loc=\"upper center\",\n",
    "    bbox_to_anchor=(0.5, -0.05),\n",
    "    ncol=5,\n",
    "    fontsize=10,\n",
    ")\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "fig.savefig(\n",
    "    f\"{save_figure_path}/{reference_regions}_B_bias_{index}_{period.start}-{period.stop}.png\",\n",
    "    bbox_inches=\"tight\",\n",
    "    transparent=True,\n",
    "    pad_inches=0,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
